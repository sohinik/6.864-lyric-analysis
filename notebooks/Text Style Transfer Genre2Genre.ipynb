{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Text Style Transfer Genre2Genre.ipynb","provenance":[{"file_id":"19gZVGeIsJt8Yg0RJe4mCDlJqARBWGkgF","timestamp":1620585168701}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XeRZX1sxa6pY"},"source":["# 0. Install Dependencies and Dataset"]},{"cell_type":"markdown","metadata":{"id":"2Yp9aMltwnAy"},"source":["## 0.1 Install Python packages"]},{"cell_type":"code","metadata":{"id":"NlwvkUW6YLx1","executionInfo":{"status":"ok","timestamp":1621489882910,"user_tz":240,"elapsed":15234,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["%%bash\n","rm -rf sample_data\n","\n","pip -q install transformers\n","pip -q install datasets\n","pip -q install tqdm\n","pip -q install sentencepiece \n","pip -q install kaggle\n","pip -q install sacrebleu"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgHwRIvIwrX0"},"source":["## 0.2 Download the dataset using the Kaggle API"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKJtKtWarZXm","executionInfo":{"status":"ok","timestamp":1621489887373,"user_tz":240,"elapsed":19653,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}},"outputId":"5f483935-5f3c-4a15-ac4a-1419f920af46"},"source":["%%bash\n","mkdir ~/.kaggle/\n","cd ~/.kaggle/\n","touch kaggle.json\n","chmod 600 /root/.kaggle/kaggle.json\n","echo \"{\\\"username\\\":\\\"cwcrystal8\\\",\\\"key\\\":\\\"75ba9516cfea9c5de8e657080f7428bd\\\"}\" > kaggle.json\n","\n","cd /content/\n","\n","kaggle datasets download -d mateibejan/multilingual-lyrics-for-genre-classification\n","unzip multilingual-lyrics-for-genre-classification.zip\n","rm test.csv\n","rm multilingual-lyrics-for-genre-classification.zip\n","mv train.csv data.csv"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading multilingual-lyrics-for-genre-classification.zip to /content\n","\n","Archive:  multilingual-lyrics-for-genre-classification.zip\n","  inflating: test.csv                \n","  inflating: train.csv               \n"],"name":"stdout"},{"output_type":"stream","text":["mkdir: cannot create directory ‘/root/.kaggle/’: File exists\n","\r  0%|          | 0.00/103M [00:00<?, ?B/s]\r  9%|8         | 9.00M/103M [00:00<00:01, 60.1MB/s]\r 24%|##4       | 25.0M/103M [00:00<00:01, 68.5MB/s]\r 40%|###9      | 41.0M/103M [00:00<00:00, 82.9MB/s]\r 48%|####7     | 49.0M/103M [00:00<00:00, 80.5MB/s]\r 63%|######3   | 65.0M/103M [00:00<00:00, 94.8MB/s]\r 81%|########  | 83.0M/103M [00:00<00:00, 111MB/s] \r100%|##########| 103M/103M [00:00<00:00, 124MB/s] \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"tKvbcgZuwvPn"},"source":["## 0.3 Clone the Github repo"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbHactG6rb97","executionInfo":{"status":"ok","timestamp":1621489887830,"user_tz":240,"elapsed":20098,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}},"outputId":"53558365-acc0-4977-c7ca-da2d47a640fb"},"source":["%%bash\n","cd /content/\n","!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n","rm -rf 6.864-lyric-analysis\n","git clone \"https://github.com/sohinik/6.864-lyric-analysis.git\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into '6.864-lyric-analysis'...\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"L5fFRDF0fcAd"},"source":["# 1. Clean and Format the Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BaP9AJero8pL","executionInfo":{"status":"ok","timestamp":1621489888285,"user_tz":240,"elapsed":20543,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}},"outputId":"563f6f00-85d2-4355-e85a-3dfeea564082"},"source":["import numpy as np\n","import os\n","import random\n","import torch\n","from torch import cuda\n","\n","os.chdir(\"/content/6.864-lyric-analysis\")\n","print(\"Current Working Directory:\", os.getcwd())\n","\n","seed = 0\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","if cuda.is_available():\n","  device = 'cuda'\n","  torch.cuda.manual_seed_all(seed)\n","else:\n","  print('WARNING: you are running this assignment on a cpu!')\n","  device = 'cpu'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Current Working Directory: /content/6.864-lyric-analysis\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VKGBh9jZnmxR"},"source":["## 1.1 Get data from dataset"]},{"cell_type":"code","metadata":{"id":"5xp1NJE-ocoe","executionInfo":{"status":"ok","timestamp":1621489893409,"user_tz":240,"elapsed":25660,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["import utils\n","from data_processing import get_data\n","from utils import save_model, load_model\n","\n","## Data Hyperparameters\n","clean_genre=True\n","genres=[\"Metal\", \"Jazz\"]\n","num_included=None\n","num_words_per_stanza = None # we don't want to separate stanzas since we need the entire song\n","training_ratio = 0.9\n","\n","raw_data, train_dict, test_dict = get_data(\"../data.csv\", \n","                                 clean_genre = clean_genre,\n","                                 genres = genres,\n","                                 num_included = num_included,\n","                                 num_words_per_stanza = num_words_per_stanza,\n","                                 training_ratio = training_ratio)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hJ7KZxFdnq8N"},"source":["## 1.2 Get set of lyrics separated by newlines"]},{"cell_type":"code","metadata":{"id":"CNcWCxSncSjC","executionInfo":{"status":"ok","timestamp":1621489894004,"user_tz":240,"elapsed":26250,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["def get_lyric_set(lyrics):\n","  return set([line.lower() for song in lyrics for line in song.split(\"\\n\") if len(line.strip(\"(),./\\\\|{}<>-_`~'\\\":;?!#@$%^&*\")) and 0 < song.strip().find(\"\\n\")])\n","\n","def split_by_genre(lyrics, labels):\n","  genre_lyrics = {}\n","  for i in range(len(lyrics)):\n","    label = labels[i]\n","    if label in genre_lyrics:\n","      genre_lyrics[label].append(lyrics[i])\n","    else:\n","      genre_lyrics[label] = [lyrics[i]]\n","  for genre in genre_lyrics:\n","    genre_lyrics[genre] = get_lyric_set(genre_lyrics[genre])\n","  return genre_lyrics\n","\n","genre_lyrics_train = split_by_genre(train_dict['lyrics'], train_dict['labels'])\n","genre_lyrics_test = split_by_genre(test_dict['lyrics'], test_dict['labels'])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2CYH2Bkqhd0","executionInfo":{"status":"ok","timestamp":1621489894157,"user_tz":240,"elapsed":26397,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["# train_dict['lyrics'] = [line.lower() for song in train_dict['lyrics'] for line in song.split(\"\\n\") if len(line.strip(\"(),./\\\\|{}<>-_`~'\\\":;?!#@$%^&*\")) and 0 < song.strip().find(\"\\n\")]\n","# test_dict['lyrics'] = [line.lower() for song in test_dict['lyrics'] for line in song.split(\"\\n\") if len(line.strip(\"(),./\\\\|{}<>-_`~'\\\":;?!#@$%^&*\")) and 0 < song.strip().find(\"\\n\")]\n","\n","genre_lyrics_train['Jazz'] = random.sample(genre_lyrics_train['Jazz'], 12000)\n","genre_lyrics_train['Metal'] = random.sample(genre_lyrics_train['Metal'], 12000)\n","genre_lyrics_test['Jazz'] = random.sample(genre_lyrics_test['Jazz'], 3000)\n","genre_lyrics_test['Metal'] = random.sample(genre_lyrics_test['Metal'], 3000)\n","\n","# train_dict['lyrics'] = train_dict['lyrics'][:15000]\n","# test_dict['lyrics'] = test_dict['lyrics'][:10000]"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RRPVoxrioCm2"},"source":["## 1.3 Define Dataset class"]},{"cell_type":"code","metadata":{"id":"EqFAiSL0pezG","executionInfo":{"status":"ok","timestamp":1621489894158,"user_tz":240,"elapsed":26393,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["from torch.utils import data\n","from collections import Counter\n","from torch.nn import functional as F\n","\n","# These IDs are reserved.\n","PAD_INDEX = 0\n","UNK_INDEX = 1\n","SOS_INDEX = 2\n","EOS_INDEX = 3\n","\n","MAX_SENT_LENGTH_PLUS_SOS_EOS = 100 # from max of folk and metal max lengths\n","\n","def get_vocab(data_list, size=7000):\n","  word_freq = Counter(word.strip(\"(),./\\\\|{}<>-_`~'\\\":;?!#@$%^&*\").lower() for lyrics in data_list for word in lyrics.split())\n","  del word_freq[\"\"]\n","  # print(word_freq.most_common(20))\n","  return list(pair[0] for pair in word_freq.most_common(size))\n","\n","class SongDataset(data.Dataset):\n","  def __init__(self, songs, vocab, max_size=None):\n","    self.songs = list(songs)\n","    self.max_src_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n","    self.v2id = {v : i + 4 for i, v in enumerate(vocab)}\n","    self.v2id['<pad>'] = PAD_INDEX\n","    self.v2id['<unk>'] = UNK_INDEX\n","    self.v2id['<sos>'] = SOS_INDEX\n","    self.v2id['<eos>'] = EOS_INDEX\n","    self.vocab = vocab\n","    self.vocab_size = len(self.v2id)\n","    self.id2v = {val : key for key, val in self.v2id.items()}\n","\n","  def __len__(self):\n","    return len(self.songs)\n","\n","  def __getitem__(self, index):\n","    return self.get_items(self.songs[index])\n","\n","  def get_items(self, s):\n","    song = s.lower().split()\n","    song_len = len(song) + 2   # add <s> and </s> to each sentence\n","    song_ids = []\n","    for word in song:\n","      w = word.strip(\"(),./\\\\|{}<>-_`~'\\\":;?!#@$%^&*\")\n","      if not len(w):\n","        song_len -= 1\n","        continue\n","      if w not in self.vocab:\n","        w = '<unk>'\n","      song_ids.append(self.v2id[w])\n","    song_ids = ([SOS_INDEX] + song_ids + [EOS_INDEX] + [PAD_INDEX] *\n","              (self.max_src_seq_length - song_len))\n","    attn_mask = ([1] * (song_len) + [0] * (self.max_src_seq_length - song_len))\n","\n","    song_ids = torch.tensor(song_ids)\n","\n","    song_vecs = F.one_hot(song_ids, num_classes=self.vocab_size).float()\n","\n","    return song_vecs[:100], song_ids[:100], torch.tensor(min(song_len, 100)), torch.tensor(attn_mask[:100])"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_P9RXTM7oGJP"},"source":["## 1.4 Make datasets for each genre"]},{"cell_type":"code","metadata":{"id":"WUtsKxjTrys_","executionInfo":{"status":"ok","timestamp":1621489894293,"user_tz":240,"elapsed":26523,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["vocab = get_vocab(genre_lyrics_train['Jazz'] + genre_lyrics_train['Metal'] + genre_lyrics_test['Jazz'] + genre_lyrics_test['Metal'])\n","datasets_train = {genre: SongDataset(lyrics, vocab) for genre, lyrics in genre_lyrics_train.items()}\n","datasets_test = {genre: SongDataset(lyrics, vocab) for genre, lyrics in genre_lyrics_test.items()}\n","\n","# jazz_dataset_train = datasets_train[\"Jazz\"]\n","# jazz_dataset_test = datasets_test[\"Jazz\"]\n","# metal_dataset_train = datasets_train[\"Metal\"]\n","# metal_dataset_test = datasets_test[\"Metal\"]"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FRTZ_UBqw5V1"},"source":["# 2. Design Model\n"]},{"cell_type":"markdown","metadata":{"id":"brAM_EmYZXkd"},"source":["## 2.1 Encoder "]},{"cell_type":"code","metadata":{"id":"VzATy2vIG2Tp","executionInfo":{"status":"ok","timestamp":1621489894293,"user_tz":240,"elapsed":26518,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class Encoder(nn.Module):\n","    \"\"\"Encodes a sequence of word embeddings\"\"\"\n","    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n","        super(Encoder, self).__init__()\n","        self.num_layers = num_layers\n","        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n","                          batch_first=True, bidirectional=True, dropout=dropout)\n","        # self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n","        #                   batch_first=True, bidirectional=False, dropout=dropout)\n","        \n","    def forward(self, x, mask, lengths):\n","        \"\"\"\n","        Applies a bidirectional GRU to sequence of embeddings x.\n","        The input mini-batch x needs to be sorted by length.\n","        x should have dimensions [batch, time, dim].\n","        \"\"\"\n","        packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n","        output, final = self.rnn(packed)\n","        output, _ = pad_packed_sequence(output, batch_first=True, total_length=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","\n","        # we need to manually concatenate the final states for both directions\n","        fwd_final = final[0:final.size(0):2]\n","        bwd_final = final[1:final.size(0):2]\n","        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n","\n","        return output, final"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9UndOKsPZb_h"},"source":["## 2.2 Decoder"]},{"cell_type":"code","metadata":{"id":"SbCeBBw_G44-","executionInfo":{"status":"ok","timestamp":1621489894743,"user_tz":240,"elapsed":26963,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["class Decoder(nn.Module):\n","    \"\"\"A conditional RNN decoder with attention.\"\"\"\n","    \n","    def __init__(self, emb_size, hidden_size, attention=None, num_layers=1, dropout=0.5,\n","                 bridge=True):\n","        super(Decoder, self).__init__()\n","        \n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.attention = attention\n","        self.dropout = dropout\n","                 \n","        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n","                          batch_first=True, dropout=dropout)\n","\n","        # self.rnn = nn.GRU(emb_size, hidden_size, num_layers,\n","        #                   batch_first=True, dropout=dropout)\n","                 \n","        # to initialize from the final encoder state\n","        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n","\n","        self.dropout_layer = nn.Dropout(p=dropout)\n","        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n","                                          hidden_size, bias=False)\n","        # self.pre_output_layer = nn.Linear(hidden_size + emb_size,\n","        #                                    hidden_size, bias=False)\n","        \n","    def forward_step(self, prev_embed, encoder_hidden, src_mask, hidden, proj_key): #proj_key\n","        \"\"\"Perform a single decoder step (1 word)\"\"\"\n","\n","\n","        # compute context vector using attention mechanism\n","        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n","\n","        context, attn_probs = self.attention(\n","            query=query, proj_key=proj_key,\n","            value=encoder_hidden, mask=src_mask)\n","\n","        # update rnn hidden state\n","        rnn_input = torch.cat([prev_embed, context], dim=2)\n","        output, hidden = self.rnn(rnn_input, hidden)\n","        # output, hidden = self.rnn(prev_embed, hidden)\n","        \n","        pre_output = torch.cat([prev_embed, output, context], dim=2)\n","        # pre_output = torch.cat([prev_embed, output], dim=2)\n","        pre_output = self.dropout_layer(pre_output)\n","        pre_output = self.pre_output_layer(pre_output)\n","\n","        return output, hidden, pre_output\n","    \n","    def forward(self, trg_embed, encoder_hidden, encoder_final, \n","                src_mask, trg_mask, hidden=None, max_len=None):\n","        \"\"\"Unroll the decoder one step at a time.\"\"\"\n","                                         \n","        # the maximum number of steps to unroll the RNN\n","        if max_len is None:\n","            # max_len = trg_mask.size(-1)\n","            max_len = trg_embed.size(1)\n","\n","        # initialize decoder hidden state\n","        if hidden is None:\n","            hidden = self.init_hidden(encoder_final)\n","        \n","        # pre-compute projected encoder hidden states\n","        # (the \"keys\" for the attention mechanism)\n","        # this is only done for efficiency\n","        proj_key = self.attention.key_layer(encoder_hidden)\n","        \n","        # here we store all intermediate hidden states and pre-output vectors\n","        decoder_states = []\n","        pre_output_vectors = []\n","        \n","        # unroll the decoder RNN for max_len steps\n","        for i in range(max_len):\n","\n","            # prev_embed = trg_embed[:, i].unsqueeze(1)\n","            prev_embed = trg_embed[:,i:i+1,:]\n","            pre_output, hidden, output = self.forward_step(\n","              prev_embed, encoder_hidden, src_mask, hidden, proj_key)\n","            decoder_states.append(output)\n","            pre_output_vectors.append(pre_output)\n","\n","        decoder_states = torch.cat(decoder_states, dim=1)\n","        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n","        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n","\n","    def init_hidden(self, encoder_final):\n","        \"\"\"Returns the initial decoder state,\n","        conditioned on the final encoder state.\"\"\"\n","\n","        if encoder_final is None:\n","            return None  # start with zeros\n","\n","        return torch.tanh(self.bridge(encoder_final))\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVASvICfZdoz"},"source":["## 2.3 Attention"]},{"cell_type":"code","metadata":{"id":"wR2EvlaBG9s_","executionInfo":{"status":"ok","timestamp":1621489894744,"user_tz":240,"elapsed":26957,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["class BahdanauAttention(nn.Module):\n","    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n","    \n","    def __init__(self, hidden_size, key_size=None, query_size=None):\n","        super(BahdanauAttention, self).__init__()\n","        \n","        # We assume a bi-directional encoder so key_size is 2*hidden_size\n","        key_size = 2 * hidden_size if key_size is None else key_size\n","        query_size = hidden_size if query_size is None else query_size\n","\n","        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n","        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n","        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n","        \n","        # to store attention scores\n","        self.alphas = None\n","        \n","    def forward(self, query=None, proj_key=None, value=None, mask=None):\n","        assert mask is not None, \"mask is required\"\n","        \n","        # We first project the query (the decoder state).\n","        # The projected keys (the encoder states) were already pre-computated.\n","        query = self.query_layer(query)\n","        \n","        # Calculate scores.\n","        scores = self.energy_layer(torch.tanh(query + proj_key))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","        \n","        # Mask out invalid positions.\n","        # The mask marks valid positions so we invert it using `mask & 0`.\n","        scores.data.masked_fill_(mask == 0, -float('inf'))\n","        \n","        # Turn scores to probabilities.\n","        alphas = F.softmax(scores, dim=-1)\n","        self.alphas = alphas        \n","        \n","        # The context vector is the weighted sum of the values.\n","        context = torch.bmm(alphas, value)\n","        \n","        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n","        return context, alphas"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZelcGCyZgfh"},"source":["## 2.4 EncoderDecoderLyricGenerator"]},{"cell_type":"code","metadata":{"id":"XzJXUO9EHH63","executionInfo":{"status":"ok","timestamp":1621489894746,"user_tz":240,"elapsed":26952,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["class EncoderDecoderLyricGenerator(nn.Module):\n","    \"\"\"\n","    A standard Encoder-Decoder architecture. Base for this and many \n","    other models.\n","    \"\"\"\n","\n","    def __init__(self, encoders, decoders, generators, src_embeds, trg_embeds):\n","        '''\n","        Assumes that genres are jazz and metal.\n","        '''\n","        super(EncoderDecoderLyricGenerator, self).__init__()\n","        self.jazz_encoder = encoders['Jazz']\n","        self.metal_encoder = encoders['Metal']\n","        self.jazz_decoder = decoders['Jazz']\n","        self.metal_decoder = decoders['Metal']\n","        self.jazz_generator = generators['Jazz']\n","        self.metal_generator = generators['Metal']\n","        self.jazz_src_embed = src_embeds['Jazz']\n","        self.metal_src_embed = src_embeds['Metal']\n","        self.jazz_trg_embed = trg_embeds['Jazz']\n","        self.metal_trg_embed = trg_embeds['Metal']\n","        \n","    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths, src_genre, trg_genre):\n","        \"\"\"Take in and process masked src and target sequences.\"\"\"\n","        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths, src_genre)\n","        return self.decode(encoder_hidden, encoder_final, src_mask, trg[:,:-1], trg_mask, trg_genre)\n","    \n","    def encode(self, src, src_mask, src_lengths, src_genre):\n","        encoder = self.jazz_encoder if src_genre == \"Jazz\" else self.metal_encoder\n","        embed = self.jazz_src_embed if src_genre == 'Jazz' else self.metal_src_embed\n","        return encoder(embed(src), src_mask, src_lengths)\n","    \n","    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask, trg_genre,\n","               decoder_hidden=None):\n","        decoder = self.jazz_decoder if trg_genre == \"Jazz\" else self.metal_decoder\n","        embed = self.jazz_trg_embed if trg_genre == 'Jazz' else self.metal_trg_embed\n","        return decoder(embed(trg), encoder_hidden, encoder_final,\n","                            src_mask, trg_mask, hidden=decoder_hidden)\n","        \n","    def generate(self, outputs, genre):\n","        generator = self.jazz_generator if genre == \"Jazz\" else self.metal_generator\n","        return generator(outputs)\n","\n","class Generator(nn.Module):\n","        \"\"\"Define standard linear + softmax generation step.\"\"\"\n","        def __init__(self, hidden_size, vocab_size):\n","            super(Generator, self).__init__()\n","            self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n","\n","        def forward(self, x):\n","            return F.log_softmax(self.proj(x), dim=-1)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWjVqUme51Qc"},"source":["# 3. Train the Model"]},{"cell_type":"markdown","metadata":{"id":"5Ob1ed3R6X5Z"},"source":["## 3.1 Create the model"]},{"cell_type":"code","metadata":{"id":"SAVpWk1uIIM0","executionInfo":{"status":"ok","timestamp":1621489898007,"user_tz":240,"elapsed":30203,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["emb_size=256\n","hidden_size=256\n","num_layers=2\n","dropout=0.2\n","\n","vocab_size = len(vocab) + 4 # to account for tokens\n","\n","genres_for_model = frozenset(genres)\n","encoders = {genre: Encoder(emb_size, hidden_size, dropout=dropout, num_layers=num_layers) for genre in genres_for_model}\n","decoders = {genre: Decoder(emb_size, hidden_size, attention=BahdanauAttention(hidden_size),dropout=dropout,num_layers=num_layers) for genre in genres_for_model}\n","generators = {genre: Generator(hidden_size, vocab_size) for genre in genres_for_model}\n","src_embeds = {genre: nn.Linear(vocab_size, emb_size) for genre in genres_for_model}\n","trg_embeds = {genre: nn.Linear(vocab_size, emb_size) for genre in genres_for_model}\n","\n","model = EncoderDecoderLyricGenerator(\n","        encoders,\n","        decoders,\n","        generators,\n","        src_embeds,\n","        trg_embeds,\n","        ).to(device)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VPBtiKVMP1rj"},"source":["## 3.2 Define classifier model"]},{"cell_type":"code","metadata":{"id":"9cjjIIzwPlSl","executionInfo":{"status":"ok","timestamp":1621489898012,"user_tz":240,"elapsed":30202,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["import torch.nn as nn\n","\n","class ModelOutputs:\n","    def __init__(self, logits = None, loss=None):\n","        self.logits = logits\n","        self.loss = loss\n","\n","class GenreClassificationModel(nn.Module):\n","  def __init__(self, lm, num_labels, dropout=0.2, num_layers = 1, is_bidirectional = False):\n","    super(GenreClassificationModel, self).__init__()\n","    # (batch_size, num_tokens)\n","    # (batch_size, num_tokens, hidden_size)\n","    # (batch_size, 1 , hidden_size)\n","    # (batch_size, 1, num_labels)\n","\n","    self.lm = lm\n","    self.dropout = nn.Dropout(dropout)\n","    self.encoder = nn.GRU(\n","        input_size  = lm.config.hidden_size,\n","        hidden_size = lm.config.hidden_size,\n","        num_layers = num_layers,\n","        batch_first = True,\n","        bidirectional = is_bidirectional,\n","        dropout = dropout\n","        )\n","    self.classifier = nn.Linear(lm.config.hidden_size, num_labels)\n","    self.bidirectional = is_bidirectional\n","    self.num_layers = num_layers\n","    \n","\n","  def forward(self, input_ids, attn_mask, labels = None):\n","    '''\n","    Inputs;\n","    input_ids: (batch_size, num_tokens) tensor of input_ids\n","    attn_mask: (batch_size, num_tokens) tensor \n","    labels (optional): (batch_size,) tensor\n","\n","\n","    Outputs:\n","    label_logits: (batch_size, num_labels) tensor of logits\n","    '''\n","\n","    lm_outputs = self.lm(input_ids, attn_mask)\n","    hidden_states = lm_outputs.last_hidden_state\n","    \n","    hidden_states = self.dropout(hidden_states)\n","\n","    _, hidden_states = self.encoder(hidden_states)\n","\n","    if not self.bidirectional:\n","      hidden_states = hidden_states[-1]\n","    else: \n","      hidden_states = torch.sum(hidden_states[-2:], dim=0)\n","    logits = self.classifier(hidden_states)\n","\n","    loss = None\n","\n","    if labels is not None:\n","        loss_fct = nn.CrossEntropyLoss()\n","        loss = loss_fct(logits, labels)\n","    \n","    return ModelOutputs(\n","        logits = logits,\n","        loss = loss)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JKt2vu026eBd"},"source":["## 3.3 Set the hyperparameters"]},{"cell_type":"code","metadata":{"id":"NkeQv7nN6DaP","executionInfo":{"status":"ok","timestamp":1621489898022,"user_tz":240,"elapsed":30206,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["# Hyper-parameters: you could try playing with different settings\n","num_epochs = 5\n","learning_rate = 1e-3\n","batch_size = 64"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyQi_wLVzj9y"},"source":["## 3.4 Define loss functions"]},{"cell_type":"code","metadata":{"id":"9XQ5mRsKQkAd","executionInfo":{"status":"ok","timestamp":1621489898027,"user_tz":240,"elapsed":30204,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["class SelfLoss:\n","    \"\"\"A self loss compute and train function.\"\"\"\n","\n","    def __init__(self, model, criterion, opt=None):\n","      self.model = model\n","      self.criterion = criterion\n","      self.opt = opt\n","\n","    def __call__(self, x, y, norm=1, genre=None):\n","        '''\n","        x: output of our encoder-decoder (logits of each word)\n","        y: input of our encoder-decoder\n","        '''\n","        x = self.model.generate(x, genre)\n","        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n","                              y.contiguous().view(-1))\n","        loss = loss / norm\n","\n","        if self.opt is not None:\n","            loss.backward()          \n","            self.opt.step()\n","            self.opt.zero_grad()\n","\n","        return loss.data.item() * norm"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1ROoOrwpMs6","executionInfo":{"status":"ok","timestamp":1621489898032,"user_tz":240,"elapsed":30204,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["class CycleLoss:\n","    \"\"\"A cycle loss compute and train function.\"\"\"\n","\n","    def __init__(self, model, criterion, opt=None):\n","      self.model = model\n","      self.criterion = criterion\n","      self.opt = opt\n","\n","    def __call__(self, x, y, norm=1, genre=None):\n","        '''\n","        x: output of our encoder-decoder (logits of each word)\n","        y: input of our encoder-decoder\n","        '''\n","        x = self.model.generate(x, genre)\n","        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n","                              y.contiguous().view(-1))\n","        loss = loss / norm\n","\n","        if self.opt is not None:\n","            loss.backward()          \n","            self.opt.step()\n","            self.opt.zero_grad()\n","\n","        return loss.data.item() * norm"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUlMI1ENpM45","executionInfo":{"status":"ok","timestamp":1621489898142,"user_tz":240,"elapsed":30308,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["# class ClassificationLoss:\n","#     \"\"\"A classification model loss compute and train function.\"\"\"\n","\n","#     def __init__(self, classifier, criterion, opt=None):\n","#       self.classifier = classifier\n","#       self.criterion = criterion\n","#       self.opt = opt\n","\n","#     def __call__(self, x, y, norm=1):\n","#         '''\n","#         x: output of our encoder-decoder (logits of each word)\n","#         y: input of our encoder-decoder\n","#         '''\n","#         # x = self.generator(x)\n","#         # x = torch.argmax(x, dim=-1)\n","\n","#         self.classifier.eval()\n","\n","#         prediction = torch.argmax(self.classifier(x)[2:4])\n","#         loss = self.criterion(prediction, y)\n","\n","#         loss = loss / norm\n","\n","#         if self.opt is not None:\n","#             loss.backward()          \n","#             self.opt.step()\n","#             self.opt.zero_grad()\n","\n","#         return loss.data.item() * norm"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nm3wsUC2rS6J","executionInfo":{"status":"ok","timestamp":1621489898145,"user_tz":240,"elapsed":30304,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["def greedy_decode_text_style_transfer_batch(model, src_ids, src_lengths, src_mask, src_genre, trg_genre, max_len, grad=True):\n","  \"\"\"Greedily decode a sentence for EncoderDecoder. Make sure to chop off the \n","     EOS token!\"\"\"\n","\n","  batch_size = src_ids.size()[0]\n","  if not grad:\n","    with torch.no_grad():\n","      encoder_hidden, encoder_finals = model.encode(src_ids, src_mask, src_lengths, src_genre)\n","      prev_y = F.one_hot(torch.ones(batch_size, 1, dtype=torch.long).fill_(SOS_INDEX), vocab_size).to(device).float()\n","      mask = torch.ones(batch_size, 1).type_as(src_ids).to(device)\n","  else:\n","    encoder_hidden, encoder_finals = model.encode(src_ids, src_mask, src_lengths, src_genre)\n","    prev_y = F.one_hot(torch.ones(batch_size, 1, dtype=torch.long).fill_(SOS_INDEX), vocab_size).to(device).float()\n","    mask = torch.ones(batch_size, 1).type_as(src_ids).to(device)\n","\n","  output = []\n","  hidden = None\n","  \n","  # --------- Your code here --------- #\n","  for i in range(max_len):\n","    outputs, hidden, cur_output = model.decode(encoder_hidden, encoder_finals, src_mask, prev_y, mask, trg_genre, decoder_hidden=hidden)\n","    cur_output = model.generate(cur_output, trg_genre)\n","    output.append(cur_output)\n","    prev_y = cur_output\n","  # --------- Your code ends --------- #\n","\n","  return torch.cat(output, dim=1)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"ug_8au1Ky5V2","executionInfo":{"status":"ok","timestamp":1621489898147,"user_tz":240,"elapsed":30301,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["def decode_to_string(ids, dataset):\n","  words = [dataset.id2v[i.item()] for i in ids]\n","  return \" \".join(words)\n","# def decode_to_string_batch(batch_ids, dataset):\n","#   # input batch_size x max len x 1\n","#   output = []\n","#   for ids in batch_ids:\n","#     output.append(\" \".join([dataset.id2v[i.item()] for i in ids]))\n","#   # batch size tensor of strings \n","#   return output"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RciuiRuU6iYm"},"source":["## 3.5 Train"]},{"cell_type":"code","metadata":{"id":"HSQVP-jpJYDW","executionInfo":{"status":"ok","timestamp":1621489898150,"user_tz":240,"elapsed":30298,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["def run_epoch(data_loader, model, loss_compute, cycle_loss_compute, print_every, src_genre, int_genre):\n","  total_tokens = 0\n","  total_loss = []\n","\n","  for i, (song_vecs, song_ids, song_lengths, song_attn) in enumerate(data_loader):\n","\n","    song_vecs=song_vecs.to(device)\n","    song_ids = song_ids.to(device)\n","    song_attn = song_attn.to(device)\n","    song_attn = song_attn.unsqueeze(-2)\n","\n","    _, _, self_output = model(song_vecs, song_vecs, song_attn, song_attn, song_lengths, song_lengths, src_genre, src_genre)\n","\n","    loss0 = loss_compute(x=self_output, y=song_ids[:, 1:],\n","                        norm=song_vecs.size(0), genre=src_genre)\n","    \n","    cycle_pre_output = greedy_decode_text_style_transfer_batch(model, song_vecs, song_lengths, song_attn, src_genre, int_genre, MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","\n","    int_attn_mask = torch.ones(song_attn.shape).to(device)\n","    int_lengths = torch.ones(batch_size).fill_(MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","\n","    _, _, cycle_output = model(cycle_pre_output, song_vecs, int_attn_mask, song_attn, int_lengths, song_lengths, src_genre, int_genre)\n","    loss1 = cycle_loss_compute(x=cycle_output, y=song_ids[:, 1:],\n","                        norm=song_vecs.size(0), genre=src_genre)\n","\n","    # decoded_outputs = greedy_decode_text_style_transfer_batch(model, src_ids, src_lengths, src_mask, src_genre, trg_genre, max_len)\n","    # decoded_strings = decode_to_string_batch(decoded_outputs, datasets_train[trg_genre])\n","\n","    loss = (loss0 + loss1)/2\n","    total_loss.append(loss)\n","\n","    if model.training and i % print_every == 0:\n","      print(\"Epoch Step: %d Loss: %f\" % (i, loss / song_ids.size(0)))\n","\n","  return total_loss\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"NsjMFT4r6SX3","executionInfo":{"status":"ok","timestamp":1621489898283,"user_tz":240,"elapsed":30423,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["train = False\n","print_every = 50\n","\n","if train:\n","  data_loaders = {genre: data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2) for genre, dataset in datasets_train.items()}\n","  \n","  criterion = nn.NLLLoss(reduction='sum', ignore_index=PAD_INDEX)\n","  criterion_cycle = nn.NLLLoss(reduction='sum', ignore_index=PAD_INDEX)\n","  # criterion_classifier = nn.CrossEntropyLoss()\n","\n","  optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  \n","  self_loss_fn = SelfLoss(model,criterion, optim)\n","  cycle_loss_fn = CycleLoss(model,criterion, optim)\n","  # classifier_loss_fn = ClassifierLoss(classifier_model, criterion_classifier, optim)\n","\n","  # folk_losses = []\n","  jazz_losses = []\n","  metal_losses = []\n","  losses = []\n","\n","  step_id = 0\n","\n","  print(\"model training\")\n","\n","  print(len(data_loaders['Jazz']))\n","  print(len(data_loaders['Metal']))\n","\n","  for _ in range(num_epochs):\n","\n","    model.train()\n","    epoch_loss_jazz = run_epoch(data_loader=data_loaders['Jazz'], model=model,\n","                loss_compute=self_loss_fn, cycle_loss_compute=cycle_loss_fn,\n","                print_every=print_every, src_genre='Jazz', int_genre='Metal')\n","    epoch_loss_metal = run_epoch(data_loader=data_loaders['Metal'], model=model,\n","                loss_compute=self_loss_fn, cycle_loss_compute=cycle_loss_fn, \n","                print_every=print_every, src_genre='Metal', int_genre='Jazz')\n","\n","    jazz_losses.extend(epoch_loss_jazz)\n","    metal_losses.extend(epoch_loss_metal)\n","    losses.extend(epoch_loss_jazz)\n","    losses.extend(epoch_loss_metal)\n","  print('model training complete')  \n","  utils.save_model(model, \"/content/genre2genre.pt\")\n","\n","else:\n","  model.load_state_dict(torch.load(\"/content/genre2genre.pt\"))\n"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAwZ1dWIyO8v","executionInfo":{"status":"ok","timestamp":1621489898286,"user_tz":240,"elapsed":30420,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["import matplotlib.pyplot as plt\n","\n","if train:\n","\n","  plt.plot(losses)\n","  plt.legend()\n","  plt.xlabel(\"Step\")\n","  plt.ylabel(\"Loss\")\n","  plt.title(\"Step vs loss for genre-to-genre generation model\")\n","  plt.show()"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SVx55lwQG0WI"},"source":["# 4. Evaluate the Model\n"]},{"cell_type":"markdown","metadata":{"id":"zVCLkKDyJOEP"},"source":["## 4.1 Calculate the basic cross entropy loss"]},{"cell_type":"code","metadata":{"id":"f4PtWcOuw9zY","executionInfo":{"status":"ok","timestamp":1621489898288,"user_tz":240,"elapsed":30416,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["test_batch_size = 1\n","\n","datasets_test = {genre: SongDataset(lyrics, vocab) for genre, lyrics in genre_lyrics_test.items()}\n","data_loaders_test = {genre: data.DataLoader(dataset, batch_size=test_batch_size, shuffle=True) for genre, dataset in datasets_test.items()}"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Ij9hjPWiNDU"},"source":["## 4.2 Qualitative analysis using greedy decode"]},{"cell_type":"code","metadata":{"id":"tOUkNhEPG4En","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621492478887,"user_tz":240,"elapsed":35964,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}},"outputId":"30a1abe9-f6ec-4995-bb1b-a06db7347ef0"},"source":["model.eval()\n","\n","total_loss = 0\n","\n","# confusion_matrix = torch.zeros((num_labels, num_labels)).to(device)\n","num_test_batches = 0\n","\n","jazz_to_jazz = []\n","jazz_to_metal = []\n","metal_to_jazz = []\n","metal_to_metal = []\n","\n","for genre, data_loader in data_loaders_test.items():\n","\n","    int_genre = \"Jazz\" if genre == \"Metal\" else \"Metal\"\n","\n","    for i, (song_vecs, song_ids, song_lengths, song_attn) in enumerate(data_loader):\n","\n","        if i > 100:\n","          break\n","\n","        song_ids = song_ids.to(device)\n","        song_vecs = song_vecs.to(device)\n","        song_attn = song_attn.to(device)\n","\n","        # output = greedy_decode(model, song_ids, song_lengths, song_attn, MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","        output = greedy_decode_text_style_transfer_batch(model, song_vecs, song_lengths, song_attn, genre, genre, MAX_SENT_LENGTH_PLUS_SOS_EOS, grad=False)\n","        output = torch.argmax(output, dim=-1).squeeze()\n","\n","        # print(genre, genre)\n","        # print(\"input:\", decode_to_string(song_ids[0,1:song_lengths-1], datasets_test[genre]))\n","        # print(\"output:\", decode_to_string(output, datasets_test[genre]))\n","        # print()\n","        if genre == 'Jazz':\n","          jazz_to_jazz.extend(decode_to_string(output, datasets_test[genre]).split())\n","        else:\n","          metal_to_metal.extend(decode_to_string(output, datasets_test[genre]).split())\n","\n","        output = greedy_decode_text_style_transfer_batch(model, song_vecs, song_lengths, song_attn, genre, int_genre, MAX_SENT_LENGTH_PLUS_SOS_EOS, grad=False)\n","        output = torch.argmax(output, dim=-1).squeeze()\n","\n","        # print(genre, int_genre)\n","        # print(\"input:\", decode_to_string(song_ids[0,1:song_lengths-1], datasets_test[genre]))\n","        # print(\"output:\", decode_to_string(output, datasets_test[genre]))\n","        # print()\n","        if genre == 'Jazz':\n","          jazz_to_metal.extend(decode_to_string(output, datasets_test[genre]).split())\n","        else:\n","          metal_to_jazz.extend(decode_to_string(output, datasets_test[genre]).split())\n","\n","jazz_to_jazz = Counter(jazz_to_jazz)\n","jazz_to_metal = Counter(jazz_to_metal)\n","metal_to_jazz = Counter(metal_to_jazz)\n","metal_to_metal = Counter(metal_to_metal)\n","\n","print(jazz_to_jazz.most_common(10))\n","print(jazz_to_metal.most_common(10))\n","print(metal_to_jazz.most_common(10))\n","print(metal_to_metal.most_common(10))\n","\n","print()"],"execution_count":32,"outputs":[{"output_type":"stream","text":["[('till', 1298), ('there’s', 1054), ('oh', 951), ('air', 659), ('star', 648), (\"what's\", 565), ('silence', 460), ('cut', 380), ('hands', 379), ('ah', 373)]\n","[('until', 9367), ('lonely', 589), ('grace', 27), ('answer', 11), ('sound', 9), ('burn', 8), ('sad', 7), ('say', 6), ('greater', 6), ('thought', 5)]\n","[('there’s', 9970), ('murderer', 64), ('<eos>', 20), (\"what's\", 6), ('and', 5), ('the', 4), ('for', 3), ('soon', 3), ('<unk>', 2), ('down', 2)]\n","[('until', 2432), ('wanna', 1472), ('as', 1151), ('lonely', 1142), (\"can't\", 587), (\"it's\", 405), ('do', 261), ('never', 207), ('<unk>', 197), ('will', 196)]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DvyZe1fhiWkY"},"source":["## 4.3 BLEU score"]},{"cell_type":"code","metadata":{"id":"JapAq2IGiYY_","executionInfo":{"status":"ok","timestamp":1621487419333,"user_tz":240,"elapsed":47874,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["import sacrebleu\n","from tqdm import tqdm\n","\n","def compute_BLEU(model, data_loader, decoder, dataset, genre):\n","\n","  bleu_score = []\n","\n","  model.eval()\n","  for src_vecs, src_ids, src_lengths, src_mask in tqdm(data_loader):\n","    try:\n","      result = decoder(model, src_vecs.to(device), src_lengths, src_mask.to(device),\n","                          genre, genre,\n","                          max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","      \n","      result = torch.argmax(result, dim=-1).squeeze()\n","      \n","      # remove <s>\n","      src_ids = src_ids[0, 1:]\n","      # remove </s> and <pad>\n","      src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n","\n","      pred = decode_to_string(result, dataset)\n","      targ = decode_to_string(src_ids, dataset)\n","\n","      if targ:\n","        bleu_score.append(sacrebleu.raw_corpus_bleu([pred], [[targ]]).score)\n","    \n","    except:\n","      continue\n","\n","  return bleu_score"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n73_iR-9iit-","executionInfo":{"status":"ok","timestamp":1621487975187,"user_tz":240,"elapsed":603714,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}},"outputId":"f085405c-7fdb-4856-df17-46f072d1751c"},"source":["print('Jazz BLEU score: %f' % (np.mean(compute_BLEU(model, \n","                                            data_loaders_test[\"Jazz\"],\n","                                            greedy_decode_text_style_transfer_batch, \n","                                            datasets_test[\"Jazz\"], \"Jazz\"))))\n","print('Metal BLEU score: %f' % (np.mean(compute_BLEU(model, \n","                                            data_loaders_test[\"Metal\"],\n","                                            greedy_decode_text_style_transfer_batch, \n","                                            datasets_test[\"Metal\"], \"Metal\"))))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["100%|██████████| 3000/3000 [04:37<00:00, 10.83it/s]\n","  0%|          | 2/3000 [00:00<04:48, 10.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["Jazz BLEU score: 0.155308\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3000/3000 [04:38<00:00, 10.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Metal BLEU score: 0.168149\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3ThqmuC2lfM6","executionInfo":{"status":"ok","timestamp":1621487975191,"user_tz":240,"elapsed":603715,"user":{"displayName":"Tiffany Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgIkvnK4EtT0sdY5BYoLvANVPdJHmyTEOdZYha6=s64","userId":"00624223966114553931"}}},"source":["def compute_BLEU_cycle(model, data_loader, decoder, og_dataset, intermediate_dataset, genre, int_genre):\n","\n","  bleu_score = []\n","\n","  model.eval()\n","  for src_vecs, src_ids, src_lengths, src_mask in tqdm(data_loader):\n","    try:\n","      if not len(src_ids.size()):\n","        continue\n","\n","      int_result = decoder(model, src_vecs.to(device), src_lengths, src_mask.to(device), genre, int_genre,\n","                          max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","      \n","      int_result = torch.argmax(int_result, dim=-1).squeeze()\n","      int_result = decode_to_string(int_result, intermediate_dataset)\n","      \n","      new_vecs, new_ids, new_lengths, new_mask = intermediate_dataset.get_items(int_result)\n","\n","      new_vecs = new_vecs[None, :]\n","      new_ids = new_ids[None,:]\n","      new_lengths = new_lengths[None]\n","      new_mask = new_mask[None,:]\n","\n","      # print(new_ids.shape, new_mask.shape, new_lengths.shape)\n","      result = decoder(model, new_vecs.to(device), new_lengths, new_mask.to(device), genre, int_genre,\n","                          max_len=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n","      result = torch.argmax(result, dim=-1).squeeze() if len(result.shape) > 1 else result\n","      \n","      # remove <s>\n","      src_ids = src_ids[0, 1:]\n","\n","      # remove </s> and <pad>\n","      src_ids = src_ids[:np.where(src_ids == EOS_INDEX)[0][0]]\n","\n","      pred = decode_to_string(result, og_dataset)\n","      targ = decode_to_string(src_ids, og_dataset)\n","\n","      if targ:\n","        bleu_score.append(sacrebleu.raw_corpus_bleu([pred], [[targ]]).score)\n","\n","    except:\n","      continue\n","\n","  return bleu_score"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbCpYrCSlfYT","outputId":"749922fc-594f-45c5-af0b-7f209622ed82"},"source":["print('\\nJazz -> Metal -> Jazz BLEU score: %f' % (np.mean(compute_BLEU_cycle(model,\n","                                            data_loaders_test[\"Jazz\"],\n","                                            greedy_decode_text_style_transfer_batch, \n","                                            datasets_test[\"Jazz\"], datasets_test[\"Metal\"], \"Jazz\", \"Metal\"))))\n","print('\\nMetal -> Jazz -> Metal BLEU score: %f' % (np.mean(compute_BLEU_cycle(model,\n","                                            data_loaders_test[\"Metal\"],\n","                                            greedy_decode_text_style_transfer_batch, \n","                                            datasets_test[\"Metal\"], datasets_test[\"Jazz\"], \"Metal\", \"Jazz\"))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 36%|███▌      | 1065/3000 [03:20<05:53,  5.47it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"T1MLLCr5JcDW"},"source":["## 4.2 Calculate the accuracy, precision, and recall"]},{"cell_type":"code","metadata":{"id":"Jl31VByKJsZ9"},"source":["import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","\n","def get_recall(confusion_matrix):\n","  return torch.diag(confusion_matrix) / confusion_matrix.sum(dim = 1)\n","\n","def get_precision(confusion_matrix):\n","  return torch.diag(confusion_matrix) / confusion_matrix.sum(dim = 0)\n","\n","def get_accuracy(confusion_matrix):\n","  return torch.diag(confusion_matrix).sum() / confusion_matrix.sum()\n","\n","def plot_confusion_matrix(confusion_matrix):\n","  df_cm = pd.DataFrame(confusion_matrix.cpu().numpy(), \n","                     index = all_labels,\n","                     columns = all_labels)\n","\n","  plt.figure(figsize = (10,7))\n","  sn.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')  \n","  plt.ylabel(\"Actual\")\n","  plt.xlabel(\"Predicted\")\n","\n","def plot_statistics(confusion_matrix):\n","  accuracy = get_accuracy(confusion_matrix)\n","  print(\"Accuracy:\", accuracy.item())\n","\n","  recall = get_recall(confusion_matrix)\n","  precision = get_precision(confusion_matrix)\n","\n","  return pd.DataFrame({\"Precision\": precision.cpu().numpy(), \"Recall\": recall.cpu().numpy()}, index = all_labels)\n","\n","plot_confusion_matrix(confusion_matrix)\n","plot_statistics(confusion_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2xrP_fD1iyZ"},"source":["plot_statistics(confusion_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrjIMLQqRmIT"},"source":["Implementation Notes:\n","1. Split songs up into stanzas with 150 words each\n","2. Truncated it at 512 tokens (due to BERT limits)\n","3. Used GRU to combine information from each word in the song, LSTM does not improve performance\n","4. Spread across 5 genres as default\n","5. Equal number of datapoints for each genre\n","6. Different transformer types - ALBERT did not improve performance\n","\n","Ideas:\n","1. use a different embedding (google word2vec or stanford glove)\n","2. use a different genre\n","\n","TO DO:\n","1. change from stanza separation + tokenization + BERT embedding over to using entire songs, embedding them, and encoding + decoding them"]},{"cell_type":"code","metadata":{"id":"9ahaPkNWgOpt"},"source":["utils.save_model(model, \"/content/shitty_model.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mu7fGFOMllri"},"source":["while True:\n","  pass"],"execution_count":null,"outputs":[]}]}